{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea0f02c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _boston_dataset:\n",
      "\n",
      "Boston house prices dataset\n",
      "---------------------------\n",
      "\n",
      "**Data Set Characteristics:**  \n",
      "\n",
      "    :Number of Instances: 506 \n",
      "\n",
      "    :Number of Attributes: 13 numeric/categorical predictive. Median Value (attribute 14) is usually the target.\n",
      "\n",
      "    :Attribute Information (in order):\n",
      "        - CRIM     per capita crime rate by town\n",
      "        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n",
      "        - INDUS    proportion of non-retail business acres per town\n",
      "        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
      "        - NOX      nitric oxides concentration (parts per 10 million)\n",
      "        - RM       average number of rooms per dwelling\n",
      "        - AGE      proportion of owner-occupied units built prior to 1940\n",
      "        - DIS      weighted distances to five Boston employment centres\n",
      "        - RAD      index of accessibility to radial highways\n",
      "        - TAX      full-value property-tax rate per $10,000\n",
      "        - PTRATIO  pupil-teacher ratio by town\n",
      "        - B        1000(Bk - 0.63)^2 where Bk is the proportion of black people by town\n",
      "        - LSTAT    % lower status of the population\n",
      "        - MEDV     Median value of owner-occupied homes in $1000's\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "\n",
      "    :Creator: Harrison, D. and Rubinfeld, D.L.\n",
      "\n",
      "This is a copy of UCI ML housing dataset.\n",
      "https://archive.ics.uci.edu/ml/machine-learning-databases/housing/\n",
      "\n",
      "\n",
      "This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\n",
      "\n",
      "The Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\n",
      "prices and the demand for clean air', J. Environ. Economics & Management,\n",
      "vol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\n",
      "...', Wiley, 1980.   N.B. Various transformations are used in the table on\n",
      "pages 244-261 of the latter.\n",
      "\n",
      "The Boston house-price data has been used in many machine learning papers that address regression\n",
      "problems.   \n",
      "     \n",
      ".. topic:: References\n",
      "\n",
      "   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\n",
      "   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\n",
      "\n",
      "['CRIM' 'ZN' 'INDUS' 'CHAS' 'NOX' 'RM' 'AGE' 'DIS' 'RAD' 'TAX' 'PTRATIO'\n",
      " 'B' 'LSTAT']\n",
      "[[6.3200e-03 1.8000e+01 2.3100e+00 ... 1.5300e+01 3.9690e+02 4.9800e+00]\n",
      " [2.7310e-02 0.0000e+00 7.0700e+00 ... 1.7800e+01 3.9690e+02 9.1400e+00]\n",
      " [2.7290e-02 0.0000e+00 7.0700e+00 ... 1.7800e+01 3.9283e+02 4.0300e+00]\n",
      " ...\n",
      " [6.0760e-02 0.0000e+00 1.1930e+01 ... 2.1000e+01 3.9690e+02 5.6400e+00]\n",
      " [1.0959e-01 0.0000e+00 1.1930e+01 ... 2.1000e+01 3.9345e+02 6.4800e+00]\n",
      " [4.7410e-02 0.0000e+00 1.1930e+01 ... 2.1000e+01 3.9690e+02 7.8800e+00]]\n",
      "(506, 13)\n",
      "[24.  21.6 34.7 33.4 36.2 28.7 22.9 27.1 16.5 18.9 15.  18.9 21.7 20.4\n",
      " 18.2 19.9 23.1 17.5 20.2 18.2 13.6 19.6 15.2 14.5 15.6 13.9 16.6 14.8\n",
      " 18.4 21.  12.7 14.5 13.2 13.1 13.5 18.9 20.  21.  24.7 30.8 34.9 26.6\n",
      " 25.3 24.7 21.2 19.3 20.  16.6 14.4 19.4 19.7 20.5 25.  23.4 18.9 35.4\n",
      " 24.7 31.6 23.3 19.6 18.7 16.  22.2 25.  33.  23.5 19.4 22.  17.4 20.9\n",
      " 24.2 21.7 22.8 23.4 24.1 21.4 20.  20.8 21.2 20.3 28.  23.9 24.8 22.9\n",
      " 23.9 26.6 22.5 22.2 23.6 28.7 22.6 22.  22.9 25.  20.6 28.4 21.4 38.7\n",
      " 43.8 33.2 27.5 26.5 18.6 19.3 20.1 19.5 19.5 20.4 19.8 19.4 21.7 22.8\n",
      " 18.8 18.7 18.5 18.3 21.2 19.2 20.4 19.3 22.  20.3 20.5 17.3 18.8 21.4\n",
      " 15.7 16.2 18.  14.3 19.2 19.6 23.  18.4 15.6 18.1 17.4 17.1 13.3 17.8\n",
      " 14.  14.4 13.4 15.6 11.8 13.8 15.6 14.6 17.8 15.4 21.5 19.6 15.3 19.4\n",
      " 17.  15.6 13.1 41.3 24.3 23.3 27.  50.  50.  50.  22.7 25.  50.  23.8\n",
      " 23.8 22.3 17.4 19.1 23.1 23.6 22.6 29.4 23.2 24.6 29.9 37.2 39.8 36.2\n",
      " 37.9 32.5 26.4 29.6 50.  32.  29.8 34.9 37.  30.5 36.4 31.1 29.1 50.\n",
      " 33.3 30.3 34.6 34.9 32.9 24.1 42.3 48.5 50.  22.6 24.4 22.5 24.4 20.\n",
      " 21.7 19.3 22.4 28.1 23.7 25.  23.3 28.7 21.5 23.  26.7 21.7 27.5 30.1\n",
      " 44.8 50.  37.6 31.6 46.7 31.5 24.3 31.7 41.7 48.3 29.  24.  25.1 31.5\n",
      " 23.7 23.3 22.  20.1 22.2 23.7 17.6 18.5 24.3 20.5 24.5 26.2 24.4 24.8\n",
      " 29.6 42.8 21.9 20.9 44.  50.  36.  30.1 33.8 43.1 48.8 31.  36.5 22.8\n",
      " 30.7 50.  43.5 20.7 21.1 25.2 24.4 35.2 32.4 32.  33.2 33.1 29.1 35.1\n",
      " 45.4 35.4 46.  50.  32.2 22.  20.1 23.2 22.3 24.8 28.5 37.3 27.9 23.9\n",
      " 21.7 28.6 27.1 20.3 22.5 29.  24.8 22.  26.4 33.1 36.1 28.4 33.4 28.2\n",
      " 22.8 20.3 16.1 22.1 19.4 21.6 23.8 16.2 17.8 19.8 23.1 21.  23.8 23.1\n",
      " 20.4 18.5 25.  24.6 23.  22.2 19.3 22.6 19.8 17.1 19.4 22.2 20.7 21.1\n",
      " 19.5 18.5 20.6 19.  18.7 32.7 16.5 23.9 31.2 17.5 17.2 23.1 24.5 26.6\n",
      " 22.9 24.1 18.6 30.1 18.2 20.6 17.8 21.7 22.7 22.6 25.  19.9 20.8 16.8\n",
      " 21.9 27.5 21.9 23.1 50.  50.  50.  50.  50.  13.8 13.8 15.  13.9 13.3\n",
      " 13.1 10.2 10.4 10.9 11.3 12.3  8.8  7.2 10.5  7.4 10.2 11.5 15.1 23.2\n",
      "  9.7 13.8 12.7 13.1 12.5  8.5  5.   6.3  5.6  7.2 12.1  8.3  8.5  5.\n",
      " 11.9 27.9 17.2 27.5 15.  17.2 17.9 16.3  7.   7.2  7.5 10.4  8.8  8.4\n",
      " 16.7 14.2 20.8 13.4 11.7  8.3 10.2 10.9 11.   9.5 14.5 14.1 16.1 14.3\n",
      " 11.7 13.4  9.6  8.7  8.4 12.8 10.5 17.1 18.4 15.4 10.8 11.8 14.9 12.6\n",
      " 14.1 13.  13.4 15.2 16.1 17.8 14.9 14.1 12.7 13.5 14.9 20.  16.4 17.7\n",
      " 19.5 20.2 21.4 19.9 19.  19.1 19.1 20.1 19.9 19.6 23.2 29.8 13.8 13.3\n",
      " 16.7 12.  14.6 21.4 23.  23.7 25.  21.8 20.6 21.2 19.1 20.6 15.2  7.\n",
      "  8.1 13.6 20.1 21.8 24.5 23.1 19.7 18.3 21.2 17.5 16.8 22.4 20.6 23.9\n",
      " 22.  11.9]\n",
      "(506,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_boston # 보스턴 집값 데이터셋\n",
    "\n",
    "boston_dataset = load_boston()\n",
    "\n",
    "print(boston_dataset.DESCR) # 데이터셋 정보 확인\n",
    "# 506개의 데이터가 있음 13개의 입력변수, 14번째 속성이 목표변수. 집 가격임\n",
    "\n",
    "print(boston_dataset.feature_names) # 속성 이름들\n",
    "\n",
    "print(boston_dataset.data) # 입력변수들이 행렬로 나옴\n",
    "print(boston_dataset.data.shape) # 506개의 행과 13개의 열, 506개의 데이터와 13개의 속성\n",
    "\n",
    "print(boston_dataset.target) # 506개의 집 가격, 목표변수가 나옴\n",
    "print(boston_dataset.target.shape) # 차원이 506인 벡터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1db5a431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
      "0    0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n",
      "1    0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
      "2    0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n",
      "3    0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n",
      "4    0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n",
      "..       ...   ...    ...   ...    ...    ...   ...     ...  ...    ...   \n",
      "501  0.06263   0.0  11.93   0.0  0.573  6.593  69.1  2.4786  1.0  273.0   \n",
      "502  0.04527   0.0  11.93   0.0  0.573  6.120  76.7  2.2875  1.0  273.0   \n",
      "503  0.06076   0.0  11.93   0.0  0.573  6.976  91.0  2.1675  1.0  273.0   \n",
      "504  0.10959   0.0  11.93   0.0  0.573  6.794  89.3  2.3889  1.0  273.0   \n",
      "505  0.04741   0.0  11.93   0.0  0.573  6.030  80.8  2.5050  1.0  273.0   \n",
      "\n",
      "     PTRATIO       B  LSTAT  \n",
      "0       15.3  396.90   4.98  \n",
      "1       17.8  396.90   9.14  \n",
      "2       17.8  392.83   4.03  \n",
      "3       18.7  394.63   2.94  \n",
      "4       18.7  396.90   5.33  \n",
      "..       ...     ...    ...  \n",
      "501     21.0  391.99   9.67  \n",
      "502     21.0  396.90   9.08  \n",
      "503     21.0  396.90   5.64  \n",
      "504     21.0  393.45   6.48  \n",
      "505     21.0  396.90   7.88  \n",
      "\n",
      "[506 rows x 13 columns]\n"
     ]
    }
   ],
   "source": [
    "# 이제 데이터프레임으로 보자!\n",
    "import pandas as pd\n",
    "\n",
    "x = pd.DataFrame(boston_dataset.data, columns=boston_dataset.feature_names)\n",
    "print(x)\n",
    "\n",
    "# 입력변수 한 개일 때만 봐보자!\n",
    "x = x[['AGE']] # 열 하나만 갖고오기\n",
    "\n",
    "# 목표변수 설정\n",
    "y = pd.DataFrame(boston_dataset.target, columns=['MEDV'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9f593c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(404, 1)\n",
      "(102, 1)\n",
      "(404, 1)\n",
      "(102, 1)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 입력변수(x)와 목표변수(y) 넘겨주고, 전체 데이터 중에 20퍼센트만 test에 사용, 나머지는 train에 사용하라는 의미\n",
    "# random_state는 test를 어떻게 고를지 정하는 마라미터임, 옵셔널이라서 안 줘도 됨\n",
    "# 안 주면 실행할 때마다 새롭게 매번 랜덤한 test를 고르게 됨 (지금처럼 정수 넘겨주면 매번 같은 데이터 고름, 아무 정수 넣어도 됨)\n",
    "# 4개의 값을 반환해서 각각 들어갈 것을 설정했다.\n",
    "# 모두 판다스 데이터 프레임이다.\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=5) \n",
    "\n",
    "# 20대 80으로 나뉨\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "efb73dd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.12402883]]\n",
      "[31.04617413]\n",
      "67.8462187008521\n",
      "8.236881612652455\n"
     ]
    }
   ],
   "source": [
    "# 사이킷런으로 선형회귀 사용\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "print(model.coef_) # 모델의 정보중 세타1 값\n",
    "print(model.intercept_) # 세타0의 값\n",
    "\n",
    "y_test_prediction = model.predict(x_test) # 테스트 데이터셋에 대한 예측값\n",
    "\n",
    "# 평균 제곱 오차 구하기\n",
    "from sklearn.metrics import mean_squared_error\n",
    "print(mean_squared_error(y_test, y_test_prediction)) # 평균 제곱 오차\n",
    "print(mean_squared_error(y_test, y_test_prediction) ** 0.5) # 평균 제곱근 오차, 루트 씌우면 되는데, 0.5제곱과 같으니 0.5제곱 하기\n",
    "# 위 모델로 집값 구하면 약 8천달러 정도의 오차가 있다고 보면 됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "131f4a51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.30799852e-01  4.94030235e-02  1.09535045e-03  2.70536624e+00\n",
      "  -1.59570504e+01  3.41397332e+00  1.11887670e-03 -1.49308124e+00\n",
      "   3.64422378e-01 -1.31718155e-02 -9.52369666e-01  1.17492092e-02\n",
      "  -5.94076089e-01]]\n",
      "[37.91248701]\n",
      "20.86929218377074\n",
      "4.5682920423032\n"
     ]
    }
   ],
   "source": [
    "# 다중 선형회귀\n",
    "X = pd.DataFrame(boston_dataset.data, columns=boston_dataset.feature_names)\n",
    "X\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=5) \n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(model.coef_) # 세타0을 제외한 모든 세타 값\n",
    "print(model.intercept_) # 세타0의 값, 손실을 최대한 적게하는 세타 값\n",
    "\n",
    "y_test_prediction = model.predict(X_test)\n",
    "print(mean_squared_error(y_test, y_test_prediction)) # 평균 제곱 오차\n",
    "print(mean_squared_error(y_test, y_test_prediction) ** 0.5) # 평균 제곱근 오차\n",
    "# 위 모델로 집값 구하면 약 4천달러 정도의 오차가 있다고 보면 됨\n",
    "# 입력변수 여러 개 하니 오차가 줄어듦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "692ffff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506, 105)\n",
      "['1', 'CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'CRIM^2', 'CRIM ZN', 'CRIM INDUS', 'CRIM CHAS', 'CRIM NOX', 'CRIM RM', 'CRIM AGE', 'CRIM DIS', 'CRIM RAD', 'CRIM TAX', 'CRIM PTRATIO', 'CRIM B', 'CRIM LSTAT', 'ZN^2', 'ZN INDUS', 'ZN CHAS', 'ZN NOX', 'ZN RM', 'ZN AGE', 'ZN DIS', 'ZN RAD', 'ZN TAX', 'ZN PTRATIO', 'ZN B', 'ZN LSTAT', 'INDUS^2', 'INDUS CHAS', 'INDUS NOX', 'INDUS RM', 'INDUS AGE', 'INDUS DIS', 'INDUS RAD', 'INDUS TAX', 'INDUS PTRATIO', 'INDUS B', 'INDUS LSTAT', 'CHAS^2', 'CHAS NOX', 'CHAS RM', 'CHAS AGE', 'CHAS DIS', 'CHAS RAD', 'CHAS TAX', 'CHAS PTRATIO', 'CHAS B', 'CHAS LSTAT', 'NOX^2', 'NOX RM', 'NOX AGE', 'NOX DIS', 'NOX RAD', 'NOX TAX', 'NOX PTRATIO', 'NOX B', 'NOX LSTAT', 'RM^2', 'RM AGE', 'RM DIS', 'RM RAD', 'RM TAX', 'RM PTRATIO', 'RM B', 'RM LSTAT', 'AGE^2', 'AGE DIS', 'AGE RAD', 'AGE TAX', 'AGE PTRATIO', 'AGE B', 'AGE LSTAT', 'DIS^2', 'DIS RAD', 'DIS TAX', 'DIS PTRATIO', 'DIS B', 'DIS LSTAT', 'RAD^2', 'RAD TAX', 'RAD PTRATIO', 'RAD B', 'RAD LSTAT', 'TAX^2', 'TAX PTRATIO', 'TAX B', 'TAX LSTAT', 'PTRATIO^2', 'PTRATIO B', 'PTRATIO LSTAT', 'B^2', 'B LSTAT', 'LSTAT^2']\n",
      "[[-2.55720147e-07 -5.09146958e+00 -1.65753983e-01 -5.97358604e+00\n",
      "   2.43179271e+01  1.65180559e+02  2.19910116e+01  1.03167123e+00\n",
      "  -5.66895775e+00  3.22443249e+00 -1.10055942e-02  5.35127787e+00\n",
      "  -4.81524409e-02  7.53109325e-01  2.16774682e-03  2.69938772e-01\n",
      "   5.87901385e-01  2.41731932e+00 -2.52413199e-02  8.92859572e-02\n",
      "  -5.18832420e-03 -5.77807152e-02  3.55602049e-01 -3.86092282e-02\n",
      "   5.43572101e-01 -3.18134358e-04  2.40035425e-02 -7.48850220e-04\n",
      "  -7.16133310e-03 -1.06886010e-01 -1.27782609e+00  2.50137719e-02\n",
      "   1.14111417e-04 -1.25254119e-02 -4.68024813e-03  6.05725185e-04\n",
      "  -8.57873132e-03  1.85030053e-03 -4.64730601e-03  3.08484808e-02\n",
      "  -2.09065897e-01  1.30035723e+00  3.13497405e-01  6.72540164e-04\n",
      "   7.51823883e-02 -7.38014886e-03  4.23364348e-04 -6.72155117e-03\n",
      "   6.42107774e-03 -5.32275093e-03  2.43179249e+01 -1.84845896e+01\n",
      "  -6.89090796e+00  3.60375828e-02  3.05451225e+00 -4.09746374e-01\n",
      "   2.34143012e-02 -8.47140007e-01  2.67079534e-02 -4.67786369e-01\n",
      "  -4.67850812e+01  3.64543351e+00 -6.00214489e-01  1.59316284e+01\n",
      "  -9.85012970e-01  1.34091848e-01 -1.19204901e+01 -3.52741122e-02\n",
      "   1.49910251e+00  1.61796865e-01 -5.59710757e-02 -2.01415694e-02\n",
      "  -1.48173641e-01 -1.44084743e-02 -5.43970810e-01 -2.51829107e-03\n",
      "  -2.23180151e-01  1.04437606e-04 -1.11866477e-02  1.76080422e-02\n",
      "  -5.61733228e-04  7.89859009e-04 -7.29621881e-04 -6.91541692e-03\n",
      "   5.10744891e-01 -9.97148046e-02 -5.10129116e-03 -1.89041938e-01\n",
      "  -7.59517251e-03  1.03720290e-01 -1.40678180e-01  7.67704651e-03\n",
      "  -1.15933963e-01 -9.65920565e-04 -4.55543664e-02 -5.15985543e-05\n",
      "   6.37266840e-03 -1.20248657e-04 -1.90119503e-04 -1.35160919e-02\n",
      "   9.14979704e-03 -2.02259709e-04 -1.93102592e-05 -7.46677274e-04\n",
      "   9.84814764e-03]]\n",
      "[-141.89855579]\n",
      "10.217789027082967\n",
      "3.1965276515436196\n"
     ]
    }
   ],
   "source": [
    "# 다항회귀를 할 때 가상의 열(속성)을 만들어야하는데, sklearn이 모두 다 해준다!\n",
    "from sklearn.preprocessing import PolynomialFeatures # 다항 속성\n",
    "\n",
    "polynomial_transformer = PolynomialFeatures(2) # 몇 차 함수인지 작성, 2차함수로 작성함.\n",
    "polynomial_data = polynomial_transformer.fit_transform(boston_dataset.data) # 다항 변형기(다항 회귀를 위해 가공)\n",
    "\n",
    "print(polynomial_data.shape) # 506행 105열(13개 열을 조합하고 가상의 열을 추가해서 총 105개가 됨)\n",
    "polynomial_feature_names = polynomial_transformer.get_feature_names(boston_dataset.feature_names)\n",
    "print(polynomial_feature_names) # 가능한 거의 모든 2차 조합이 다 있다, 곱하거나 제곱하거나 등등.., 1은 bias임\n",
    "\n",
    "X = pd.DataFrame(polynomial_data, columns=polynomial_feature_names)\n",
    "X\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=5) \n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(model.coef_) # 세타0을 제외한 모든 세타 값\n",
    "print(model.intercept_) # 세타0의 값\n",
    "\n",
    "y_test_prediction = model.predict(X_test)\n",
    "print(mean_squared_error(y_test, y_test_prediction)) # 평균 제곱 오차\n",
    "print(mean_squared_error(y_test, y_test_prediction) ** 0.5) # 평균 제곱근 오차\n",
    "# 위 모델로 집값 구하면 약 3천달러 정도의 오차가 있다고 보면 됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "413dfe68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 2 0 2 1 0 2 0 1 1 2 2 2 0 0 2 2 0 0 1 2 0 1 1 2 1 1 1 2]\n",
      "0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "# 분류 데이터셋 iris\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris_data = load_iris()\n",
    "# print(iris_data.DESCR) # 꽃받침(sepal)과 꽃잎(petal) 길이와 너비\n",
    "\n",
    "X = pd.DataFrame(iris_data.data, columns=iris_data.feature_names)\n",
    "y = pd.DataFrame(iris_data.target, columns=['class'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=5) \n",
    "y_train = y_train.values.ravel() # 데이터 타입을 데이터프레임에서 넘파이 배열로 변경해줌\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# solver는 모델을 최적화할 때 어떤 알고리즘을 쓸지 결정하는 것이다.\n",
    "# max_iter는 최적화할 때 해당 과정을 몇 번 반복할지 결정하는 것이다. (경사하강법에서도 손실을 줄이기 위해 반복하니..)\n",
    "# 2000번 설정했다고 해서 2000번 돌진 않음, 그전에 최적화 됐다 판단하면 알아서 멈춤\n",
    "# 모두 옵셔널이라서 안 적어도 되긴 함\n",
    "model = LogisticRegression(solver='saga', max_iter=2000)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "print(model.predict(X_test))\n",
    "\n",
    "print(model.score(X_test, y_test)) # 모델 평가, 93% 확률로 제대로 분류했다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c1b57d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
